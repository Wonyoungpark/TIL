{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mldl_md.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOwXyX2DcGYRjkRUsUqCYwD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 머신러닝 & 딥러닝\n","\n","📖 \bBook\n","----\n","[혼자 공부하는 머신러닝 + 딥러닝](http://www.yes24.com/Product/Goods/96024871)\n","\n","실습 환경\n","----\n","- 구글 코랩 : [Colab](https://colab.research.google.com/)\n","- Python\n","\n","인공지능 (Artificial Intelligence)\n","----\n","사람처럼 학습하고 추론할 수 있는 지증을 가진 컴퓨터 시스템을 만드는 기술\n","- 인공일반지능(Artificial general intelligence), 강인공지능(Strong AI) : 사람과 구분하기 어려운 컴퓨터 시스템\n","- 약인공지능(Weak AI) : 음성비서, 자율주행차, 알파고\n","\n","머신러닝 (Machine Learning)\n","----\n","자동으로 데이터에서 규칙을 학습하는 알고리즘<br/>\n","지능을 구현하는 소프트웨어 담당<br/>\n","라이브러리 : scikit-learn(사이킷런)\n","\n","딥러닝 (Deep Learning)\n","----\n","머신러닝 알고리즘 중 인공 신경망을 기반으로한 방법<br/>\n","라이브러리 : TensorFlow(텐서플로), PyTorch(파이토치)\n"],"metadata":{"id":"wCoJzQPxUhw-"}},{"cell_type":"markdown","source":["# Machin Learning\n","**용어**\n","- 특성 : 데이터의 특징\n","- 모델 : 알고리즘이 구현된 객체\n","- 훈련 : 모델에 데이터를 전달해 규칙을 학습하는 과정\n","- 정확도 = (정확히 맞힌 개수) / (전체 데이터 개수)\n","- 테스트 세트 : 평가에 사용하는 데이터\n","- 훈련 세트 : 훈련에 사용하는 데이터\n","- 샘플링 편향 : 훈련 세트와 테스트 세트에 샘플링이 치우쳐짐. 전체 데이터를 대표하지 못하는 현상\n","- 데이터 전처리(data preprocessing) : 머신러닝 모델에 훈련 데이터를 주입하기 전에 가공하는 단계\n","- 표준점수 : 훈련 세트의 스케일을 변경하는 방법, 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 표현<br/>\n","  = (훈련 데이터-평균)/표준편차\n","- 브로드캐스팅 : 크기가 다른 넘파이 배열에서 자동으로 산칙연산을 모든 행/열로 확장해 수행\n","- 결정계수(R^2, coefficient of determination) : 회귀의 성능 측정 도구\n","- 과대적합 : 훈련 세트 성능 > 테스트 세트 성능\n","- 과소적합 : 훈련 세트 성능 < 테스트 세트 성능 \b| 둘 다 낮을 경우\n","- 파라미터\n","  - 모델 파라미터 : 머신러닝 알고리즘이 찾은 값 (coef_,intercept_)\n","  - 하이퍼파라미터 : 사전에 우리가 지정해야하는 값 (alpha)\n","- 모델 기반 학습 : 최적의 모델 파라미터를 찾는 훈련\n","- 훈련 기반 학습 : 훈련 세트를 저장하는 훈련 (k-최근접 이웃)\n","- 특성 공학(Feature Engineering) : 주어진 특성으로 새로운 특성을 생성\n","- 규제(Regularization) : 훈련 세트에 과대 적합되지 않도록 만듦\n","- 다중 분류 : target 데이터에 2개 이상의 클래스가 포함된 문제\n","- 시그모이드 함수(sigmoid function) : 선형 방정식의 출력을 0~1 사이 값으로 압축\n","  - 출력값>0.5 : 양성 클래스, 출력값<=0.5 : 음성 클래스\n","- 소프트맥스 함수(softmax function) : 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 함\n","- 점진적 학습, 온라인 학습 : 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 추가 훈련\n","- 에포크(epoch) : 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한번의 방법\n","- 손실 함수(loss function) : 확률적 경사 하강법이 최적화할 대상, 샘플 하나에 대한 손실을 정의\n","  - 이진 분류 : 로지스틱 회귀(이진 크로스엔트로피) 손실 함수\n","  - 다중 분류 : 크로스엔트로피 손실 함수\n","- 비용 함수(cost function) : 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합\n","- 불순도 : 결정트리가 최적의 질문을 찾기 위한 기준\n","  - 지니 불순도(gini impurity), 엔트로피 불순도(entropy impurity)\n","- 정보 이득 : 부모와 자식 노드 사이의 불순도 차이\n","- 가지치기 : 결정 트리의 성장을 제한하여 훈련 세트에 과대적합 되지 않도록 함 (max_depth)\n","- 특성 중요도 : 결정 트리에 사용된 특성이 불순도 감소에 기여한 정도\n","- 검증 세트(validation set) : 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련 세트에서 다시 떼어낸 데이터 세트\n","- 교차 검증(cross validation) : k-폴드 교차 검증, 훈련 세트를 여러 폴드로 나눠 검증 세트를 떼어 내어 평가하는 과정을 여러번 반복해 얻은 검증 점수들을 평균화\n","- 정형 데이터 : CSV, DB, Excel\n","- 비정형 데이터 : text, picture, music, NoSQL\n","- 부트스트랩 샘플(bootstrap sample) : 데이터 세트에서 중복을 허용하여 데이터를 샘플링\n","- OOB 샘플(out of bag) : 부트스트랩 샘플에 포함되지 않고 남는 샘플\n","- 히스토그램 : 구간별로 값이 발생한 빈도를 그래프로 표시\n","- 클러스터(Cluster) : 군집 알고리즘으로 모은 샘플 그룹\n","- 클러스터 중심(Cluster center = Centroid) : K-평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값\n","- 이너셔(inertia) : 클러스터 중심과 클러스터에 속한 샘플 사이의 거리, 클러스터의 샘플이 얼마나 가깝게 있는지를 나타내는 값\n","- 엘보우 방법 : 최적의 클러스터 개수를 정하는 방법, 클러스터 개수에 따라 이너셔 감소가 꺾이는 지점이 적절한 개수.\n","- 주성분(principal component) : 원본 데이터에 있는 어떤 방향, 주성분은 원본 차원과 동일, 주성분으로 바꾼 데이터는 차원이 줄어듦\n","- 설명된 분산(explained variance) : 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값\n","\n","\n","---\n","\n","\n","**알고리즘**\n","- 지도 알고리즘(supervised learning) : 훈련데이터 [ 입력데이터 & 타깃데이터 ]\n","  - 분류 : 몇 개의 클래스 중 하나로 분류\n","  - 회귀(regression) : 임의의 수치 예측, 두 변수 사이의 상관관계를 분석\n","- 비지도 알고리즘(unspervised learning) : [ 입력데이터 ], 가르쳐주지 않아도 데이터에 있는 무언가를 학습\n","  - 군집, 차원 축소\n","- k-최근접 이웃 알고리즘 : 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용\n","- 선형 회귀(Linear Regression) : 특성과 타깃 사이의 관계를 표현하는 선형 방정식\n","  - 특성이 한개 : 직선 방정식\n","  - 릿지 회귀(Ridge) : 계수를 제곱한 값으로 규제 적용 => 과대적합 완화\n","  - 라쏘 회귀(Lasso) : 계수의 절댓값을 기준으로 규제 적용\n","- 다항 회귀(Polynomial Regression) : 다항식을 사용해 특성과 타깃 사이의 관계 표시, 선형 모델로 표현 가능\n","- 다중 회귀(Multiple Regression) : 여러개의 특성을 사용한 회귀 모델\n","- 로지스틱 회귀(logistic regression) : 선형 방정식을 사용한 분류 알고리즘\n","  - 이진 분류 : 시그모이드 함수(=로지스틱스 함수)\n","  - 다중 분류 : 소프트맥스 함수(=정규화된 지수 함수)\n","- 확률적 경사 하강법(Stochastic Gradient Descent) : 훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것\n","- 미니배치 경사 하강법(minibatch gradient descent) : 무작위로 여러개의 샘플을 선택\n","- 배치 경사 하강법(batch gradient descent) : 한번에 전체 샘플을 선택해 사용\n","- 결정 트리(Decision Tree) : 질문을 추가해가며 정답을 찾아 학습하는 알고리즘\n","- 앙상블 학습(Ensemble Learning) : 더 좋은 예측 결과를 만들기 위해 여러 모델을 훈련\n","  - 랜덤 포레스트(Random Forest) : 결정 트리를 랜덤하게 만들어 숲을 구성해 각 결정 트리의 예측을 사용해 최종 예측을 만듦, 부트스트랩 샘플 사용\n","  - 엑스트라 트리(Extra Trees) : 부트스트랩 샘플을 사용하지 않고 각 결정트리를 만들때 전체 훈련 세트를 사용, 랜덤하게 노드를 분할해 과대적합을 감소시킴(splitter='random')\n","  - 그레이디언트 부스팅(Gradient Boosting) : 깊이가 얕은 결정 트리를 연속적으로 추가해 손실함수를 최소화하는 앙상블, 속도가 느림\n","  - 히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting) : 256개 구간으로 분할 후, 한 구간을 분리해 누락된 값을 위해서 사용, 그레이디언트 부스팅의 속도를 개선\n","- 군집(Clustering) : 비슷한 샘플끼리 그룹화하는 비지도 학습 작업\n","  - K-평균 군집 알고리즘 : [ 처음에 랜덤하게 클러스터 중심을 정하고 클러스터 생성 -> 클러스터의 중심을 이동해 다시 클러스터 생성 ]을 반복해 최적의 클러스터를 구성하는 알고리즘\n","- 차원 축소(Dimensionality Reduction) : 원본 데이터의 특성을 적은 수의 새로운 특성으로 변화하는 비지도 학습, 저장 공간을 줄이고 시각화&성능 향상\n","  - 주성분 분석(Principal Component Analysis) : 데이터에서 가장 분산이 큰 방향을 찾는 방법, 원본 데이터를 주성분에 투영해 새로운 특성 생성\n","\n","\n","---\n","\n","\n","**패키지**\n","- matplotlib\n","  - scatter() : 산점도를 그림\n","  - imshow() : 넘파이 배열로 저장된 이미지 출력\n","    - cmap : 이미지 색 결정 : 'gray'-흑백, 'gray_r'-흑백 반전\n","  - hist() : 히스토그램을 그림\n","\n","- scikit-learn\n","  - KNeighborsClassifier() : k-최근접 이웃 분류 모델 생성 클래스\n","  - fit(훈련할 특성, 정답 데이터) : 사이킷런 모델을 훈련\n","  - score(훈련할 특성, 정답 데이터) : 훈련된 모델의 성능 측정, 정확도 측정\n","  - predict(새 특성 데이터) : 훈련된 모델로 새로운 데이터의 정답 예측\n","  - train_test_split() : 훈련 데이터-> [훈련 세트 & 테스트세트] \n","  - kneighbors() : k-최근접 이웃 객체의 메서드, 입력한 데이터에 가장 가까운 이웃들 찾아 인덱스 반환\n","  - KNeighborsRegressor() : k-최근접 이웃 회귀 모델\n","  - mean_absolute_error(타깃값, 예측값) : 회귀 모델의 평균 절댓값 오차 계산\n","  - LinearRegression() : 선형 회귀 함수\n","    - coef_, intercept_ : 기울기, y절편\n","  - PolynomialFeatures() : 특성 공학\n","    - degree : 최고 차수 지정\n","    - include_bias=False : 절편을 위한 특성 추가 안함\n","  - transform() : 변환기, 특성을 만들거나 전처리\n","  - StandardScaler() : 표준 점수로 변환\n","  - Ridge() : 규제가 있는 회귀 알고리즘인 릿지 회귀 모델 훈련\n","  - Lasso() : 규제가 있는 회귀 알고리즘인 라쏘 회귀 모델 훈련\n","    - alpha : 규제 강도 조절\n","  - LogisticRegression() : 로지스틱 회귀 함수\n","    - predict_proba() : 예측 확률 반환\n","    - decision_function() : z값 반환, 선형방정식의 출력 반환\n","  - SGDClassifier() : 확률적 경사 하강법을 사용한 분류 모델\n","    - loss : 최적화할 손실 함수 지정\n","    - max_iter : 에포크 횟수 지정\n","  - DecisionTreeClassifier() : 결정 트리 분류 클래스\n","    - max_depth : 트리가 성장할 최대 깊이\n","    - feature_importances : 특성 중요도\n","  - plot_tree() : 결정 트리 모델 시각화\n","  - cross_validate(검증할 모델 객체, 특성, 타겟) : 교차 검증 수행 함수\n","    - cv : 교차 검증 폴드 수/splitter 객체 지정\n","    - n_jobs : 1이면 하나의 cpu 코어 사용, -1이면 시스템의 모든 코어 사용\n","  - GridSearchCV(탐색할 모델 객체, 탐색할 모델의 매개변수와 값) : 교차 검증으로 하이퍼파라미터 탐색 수행, 최상의 모델을 찾은 후 훈련 세트 전체를 사용해 최종 모델을 훈련\n","    - best_params : 그리드 서치로 찾은 최적의 매개변수\n","  - RandomizedSearchCV() : 연속된 매개변수 값을 탐색할 때 유용, 샘플링할 수 있는 확률 분포 객체를 전달\n","  - RandomForestClassifier() : 랜덤 포레스트 분류 클래스\n","    - oob_score : OOB 샘플을 사용해 훈련한 모델을 평가할지 지정\n","  - ExtraTreesClassifier() : 엑스트라 트리 분류 클래스\n","  - GradientBoostingClassifier() : 그레이디언트 부스팅 분류 클래스\n","    - n_estimators : 부스팅 단계를 수행하는 트리 수\n","    - learning_rate : 트리가 앙상블에 기여하는 정도\n","  - HistGradientBoostingClassifier(), XGBClassifier(), LGBMClassifier() : 히스트그램 기반 그레이디언트 부스팅 분류 클래스\n","  - permutation_importance() : 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰해 어떤 특성이 중요한지 계산\n","  - KMeans : K-평균 알고리즘 클래스\n","    - n_clusters : 클러스터 개수 지정\n","    - n_init : 반복 횟수 지정\n","    - max_iter : 알고리즘 최대 반복 횟수\n","    - labels_ : 군집된 결과 저장\n","  - PCA : 주성분 분석을 수행하는 클래스\n","    - n_components : 주성분 개수 지정\n","    - explained_variance_ : 설명된 분산 지정\n","    - explained_variance_ratio_ : 설명된 분산의 비율 지정\n","    - inversed_transform() : transform()으로 차원을 축소시킨 데이터 복원\n","\n","- numpy\n","  - seed() : 난수 생성을 위한 정수 초깃값 지정, 초기값이 같으면 동일한 난수 추출 가능\n","  - shuffle(): 주어진 배열을 랜덤하게 섞음\n","  - arrange() : 일정한 간격의 정수/실수 배열 생성\n","  - column_stack(), concatenate() : 전달받은 리스트들을 연결\n","  - reshape() : 배열의 크기 변환\n","  - load() : 넘파이에 파일을 로드\n","\n","- scipy\n","  - expit() : 시그모이드 함수\n","  - uniform() : 주어진 범위에서 실숫값을 무작위로 뽑음\n","  - \brandint() : 주어진 범위에서 정숫값을 무작위로 뽑음\n","\n","\n","\n","---\n","\n","\n","**실습**\n","\n","- [k-최근접 이웃 알고리즘](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/_BreamAndSmelt.ipynb)\n","- [훈련 세트와 테스트 세트](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/training_test_data.ipynb)\n","- [데이터 전처리](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/data_preprocessing.ipynb)\n","- [k-최근접 이웃 회귀](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/regression.ipynb)\n","- [선형 회귀](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/linear_regression.ipynb)\n","- [다중 회귀_특성 공학](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/multiple_regression.ipynb)\n","- [로지스틱 회귀](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/logistic_regression.ipynb)\n","- [확률적 경사 하강법](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/stochastic_Gradient_Descent.ipynb)\n","- [결정 트리](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/decision_tree.ipynb)\n","- [교차 검증과 그리드 서치](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/cross_grid.ipynb)\n","- [앙상블 학습](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/ensemble_learning.ipynb)\n","- [군집 알고리즘](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/_clustering.ipynb)\n","- [k-평균](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/k_means.ipynb)\n","- [주성분 분석](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/_pca.ipynb)"],"metadata":{"id":"sGq5FDGq1EVk"}}]}