{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCoJzQPxUhw-"
   },
   "source": [
    "# 머신러닝 & 딥러닝\n",
    "\n",
    "📖 Book\n",
    "----\n",
    "[혼자 공부하는 머신러닝 + 딥러닝](http://www.yes24.com/Product/Goods/96024871)\n",
    "\n",
    "실습 환경\n",
    "----\n",
    "- 구글 코랩 : [Colab](https://colab.research.google.com/)\n",
    "- Python\n",
    "\n",
    "인공지능 (Artificial Intelligence)\n",
    "----\n",
    "사람처럼 학습하고 추론할 수 있는 지증을 가진 컴퓨터 시스템을 만드는 기술\n",
    "- 인공일반지능(Artificial general intelligence), 강인공지능(Strong AI) : 사람과 구분하기 어려운 컴퓨터 시스템\n",
    "- 약인공지능(Weak AI) : 음성비서, 자율주행차, 알파고\n",
    "\n",
    "머신러닝 (Machine Learning)\n",
    "----\n",
    "자동으로 데이터에서 규칙을 학습하는 알고리즘<br/>\n",
    "지능을 구현하는 소프트웨어 담당<br/>\n",
    "라이브러리 : scikit-learn(사이킷런)\n",
    "\n",
    "딥러닝 (Deep Learning)\n",
    "----\n",
    "머신러닝 알고리즘 중 인공 신경망을 기반으로한 방법<br/>\n",
    "라이브러리 : TensorFlow(텐서플로), PyTorch(파이토치)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGq5FDGq1EVk"
   },
   "source": [
    "# ***Machine Learning***\n",
    "\n",
    "**용어**\n",
    "- 특성 : 데이터의 특징\n",
    "- 모델 : 알고리즘이 구현된 객체\n",
    "- 훈련 : 모델에 데이터를 전달해 규칙을 학습하는 과정\n",
    "- 정확도 = (정확히 맞힌 개수) / (전체 데이터 개수)\n",
    "- 테스트 세트 : 평가에 사용하는 데이터\n",
    "- 훈련 세트 : 훈련에 사용하는 데이터\n",
    "- 샘플링 편향 : 훈련 세트와 테스트 세트에 샘플링이 치우쳐짐. 전체 데이터를 대표하지 못하는 현상\n",
    "- 데이터 전처리(data preprocessing) : 머신러닝 모델에 훈련 데이터를 주입하기 전에 가공하는 단계\n",
    "- 표준점수 : 훈련 세트의 스케일을 변경하는 방법, 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 표현<br/>\n",
    "  = (훈련 데이터-평균)/표준편차\n",
    "- 브로드캐스팅 : 크기가 다른 넘파이 배열에서 자동으로 산칙연산을 모든 행/열로 확장해 수행\n",
    "- 결정계수(R^2, coefficient of determination) : 회귀의 성능 측정 도구\n",
    "- 과대적합 : 훈련 세트 성능 > 테스트 세트 성능\n",
    "- 과소적합 : 훈련 세트 성능 < 테스트 세트 성능 \b| 둘 다 낮을 경우\n",
    "- 파라미터\n",
    "  - 모델 파라미터 : 머신러닝 알고리즘이 찾은 값 (coef_,intercept_)\n",
    "  - 하이퍼파라미터 : 사전에 우리가 지정해야하는 값 (alpha)\n",
    "- 모델 기반 학습 : 최적의 모델 파라미터를 찾는 훈련\n",
    "- 훈련 기반 학습 : 훈련 세트를 저장하는 훈련 (k-최근접 이웃)\n",
    "- 특성 공학(Feature Engineering) : 주어진 특성으로 새로운 특성을 생성\n",
    "- 규제(Regularization) : 훈련 세트에 과대 적합되지 않도록 만듦\n",
    "- 다중 분류 : target 데이터에 2개 이상의 클래스가 포함된 문제\n",
    "- 시그모이드 함수(sigmoid function) : 선형 방정식의 출력을 0~1 사이 값으로 압축\n",
    "  - 출력값>0.5 : 양성 클래스, 출력값<=0.5 : 음성 클래스\n",
    "- 소프트맥스 함수(softmax function) : 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 함\n",
    "- 점진적 학습, 온라인 학습 : 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 추가 훈련\n",
    "- 에포크(epoch) : 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한번의 방법\n",
    "- 손실 함수(loss function) : 확률적 경사 하강법이 최적화할 대상, 샘플 하나에 대한 손실을 정의\n",
    "  - 이진 분류 : 로지스틱 회귀(이진 크로스엔트로피) 손실 함수\n",
    "  - 다중 분류 : 크로스엔트로피 손실 함수\n",
    "- 비용 함수(cost function) : 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합\n",
    "- 불순도 : 결정트리가 최적의 질문을 찾기 위한 기준\n",
    "  - 지니 불순도(gini impurity), 엔트로피 불순도(entropy impurity)\n",
    "- 정보 이득 : 부모와 자식 노드 사이의 불순도 차이\n",
    "- 가지치기 : 결정 트리의 성장을 제한하여 훈련 세트에 과대적합 되지 않도록 함 (max_depth)\n",
    "- 특성 중요도 : 결정 트리에 사용된 특성이 불순도 감소에 기여한 정도\n",
    "- 검증 세트(validation set) : 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련 세트에서 다시 떼어낸 데이터 세트\n",
    "- 교차 검증(cross validation) : k-폴드 교차 검증, 훈련 세트를 여러 폴드로 나눠 검증 세트를 떼어 내어 평가하는 과정을 여러번 반복해 얻은 검증 점수들을 평균화\n",
    "- 정형 데이터 : CSV, DB, Excel\n",
    "- 비정형 데이터 : text, picture, music, NoSQL\n",
    "- 부트스트랩 샘플(bootstrap sample) : 데이터 세트에서 중복을 허용하여 데이터를 샘플링\n",
    "- OOB 샘플(out of bag) : 부트스트랩 샘플에 포함되지 않고 남는 샘플\n",
    "- 히스토그램 : 구간별로 값이 발생한 빈도를 그래프로 표시\n",
    "- 클러스터(Cluster) : 군집 알고리즘으로 모은 샘플 그룹\n",
    "- 클러스터 중심(Cluster center = Centroid) : K-평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값\n",
    "- 이너셔(inertia) : 클러스터 중심과 클러스터에 속한 샘플 사이의 거리, 클러스터의 샘플이 얼마나 가깝게 있는지를 나타내는 값\n",
    "- 엘보우 방법 : 최적의 클러스터 개수를 정하는 방법, 클러스터 개수에 따라 이너셔 감소가 꺾이는 지점이 적절한 개수.\n",
    "- 주성분(principal component) : 원본 데이터에 있는 어떤 방향, 주성분은 원본 차원과 동일, 주성분으로 바꾼 데이터는 차원이 줄어듦\n",
    "- 설명된 분산(explained variance) : 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**알고리즘**\n",
    "- 지도 알고리즘(supervised learning) : 훈련데이터 [ 입력데이터 & 타깃데이터 ]\n",
    "  - 분류 : 몇 개의 클래스 중 하나로 분류\n",
    "  - 회귀(regression) : 임의의 수치 예측, 두 변수 사이의 상관관계를 분석\n",
    "- 비지도 알고리즘(unspervised learning) : [ 입력데이터 ], 가르쳐주지 않아도 데이터에 있는 무언가를 학습\n",
    "  - 군집, 차원 축소\n",
    "- k-최근접 이웃 알고리즘 : 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용\n",
    "- 선형 회귀(Linear Regression) : 특성과 타깃 사이의 관계를 표현하는 선형 방정식\n",
    "  - 특성이 한개 : 직선 방정식\n",
    "  - 릿지 회귀(Ridge) : 계수를 제곱한 값으로 규제 적용 => 과대적합 완화\n",
    "  - 라쏘 회귀(Lasso) : 계수의 절댓값을 기준으로 규제 적용\n",
    "- 다항 회귀(Polynomial Regression) : 다항식을 사용해 특성과 타깃 사이의 관계 표시, 선형 모델로 표현 가능\n",
    "- 다중 회귀(Multiple Regression) : 여러개의 특성을 사용한 회귀 모델\n",
    "- 로지스틱 회귀(logistic regression) : 선형 방정식을 사용한 분류 알고리즘\n",
    "  - 이진 분류 : 시그모이드 함수(=로지스틱스 함수)\n",
    "  - 다중 분류 : 소프트맥스 함수(=정규화된 지수 함수)\n",
    "- 확률적 경사 하강법(Stochastic Gradient Descent) : 훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것\n",
    "- 미니배치 경사 하강법(minibatch gradient descent) : 무작위로 여러개의 샘플을 선택\n",
    "- 배치 경사 하강법(batch gradient descent) : 한번에 전체 샘플을 선택해 사용\n",
    "- 결정 트리(Decision Tree) : 질문을 추가해가며 정답을 찾아 학습하는 알고리즘\n",
    "- 앙상블 학습(Ensemble Learning) : 더 좋은 예측 결과를 만들기 위해 여러 모델을 훈련\n",
    "  - 랜덤 포레스트(Random Forest) : 결정 트리를 랜덤하게 만들어 숲을 구성해 각 결정 트리의 예측을 사용해 최종 예측을 만듦, 부트스트랩 샘플 사용\n",
    "  - 엑스트라 트리(Extra Trees) : 부트스트랩 샘플을 사용하지 않고 각 결정트리를 만들때 전체 훈련 세트를 사용, 랜덤하게 노드를 분할해 과대적합을 감소시킴(splitter='random')\n",
    "  - 그레이디언트 부스팅(Gradient Boosting) : 깊이가 얕은 결정 트리를 연속적으로 추가해 손실함수를 최소화하는 앙상블, 속도가 느림\n",
    "  - 히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting) : 256개 구간으로 분할 후, 한 구간을 분리해 누락된 값을 위해서 사용, 그레이디언트 부스팅의 속도를 개선\n",
    "- 군집(Clustering) : 비슷한 샘플끼리 그룹화하는 비지도 학습 작업\n",
    "  - K-평균 군집 알고리즘 : [ 처음에 랜덤하게 클러스터 중심을 정하고 클러스터 생성 -> 클러스터의 중심을 이동해 다시 클러스터 생성 ]을 반복해 최적의 클러스터를 구성하는 알고리즘\n",
    "- 차원 축소(Dimensionality Reduction) : 원본 데이터의 특성을 적은 수의 새로운 특성으로 변화하는 비지도 학습, 저장 공간을 줄이고 시각화&성능 향상\n",
    "  - 주성분 분석(Principal Component Analysis) : 데이터에서 가장 분산이 큰 방향을 찾는 방법, 원본 데이터를 주성분에 투영해 새로운 특성 생성\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**패키지**\n",
    "- matplotlib\n",
    "  - scatter() : 산점도를 그림\n",
    "  - imshow() : 넘파이 배열로 저장된 이미지 출력\n",
    "    - cmap : 이미지 색 결정 : 'gray'-흑백, 'gray_r'-흑백 반전\n",
    "  - hist() : 히스토그램을 그림\n",
    "\n",
    "- scikit-learn\n",
    "  - KNeighborsClassifier() : k-최근접 이웃 분류 모델 생성 클래스\n",
    "  - fit(훈련할 특성, 정답 데이터) : 사이킷런 모델을 훈련\n",
    "  - score(훈련할 특성, 정답 데이터) : 훈련된 모델의 성능 측정, 정확도 측정\n",
    "  - predict(새 특성 데이터) : 훈련된 모델로 새로운 데이터의 정답 예측\n",
    "  - train_test_split() : 훈련 데이터-> [훈련 세트 & 테스트세트] \n",
    "  - kneighbors() : k-최근접 이웃 객체의 메서드, 입력한 데이터에 가장 가까운 이웃들 찾아 인덱스 반환\n",
    "  - KNeighborsRegressor() : k-최근접 이웃 회귀 모델\n",
    "  - mean_absolute_error(타깃값, 예측값) : 회귀 모델의 평균 절댓값 오차 계산\n",
    "  - LinearRegression() : 선형 회귀 함수\n",
    "    - coef_, intercept_ : 기울기, y절편\n",
    "  - PolynomialFeatures() : 특성 공학\n",
    "    - degree : 최고 차수 지정\n",
    "    - include_bias=False : 절편을 위한 특성 추가 안함\n",
    "  - transform() : 변환기, 특성을 만들거나 전처리\n",
    "  - StandardScaler() : 표준 점수로 변환\n",
    "  - Ridge() : 규제가 있는 회귀 알고리즘인 릿지 회귀 모델 훈련\n",
    "  - Lasso() : 규제가 있는 회귀 알고리즘인 라쏘 회귀 모델 훈련\n",
    "    - alpha : 규제 강도 조절\n",
    "  - LogisticRegression() : 로지스틱 회귀 함수\n",
    "    - predict_proba() : 예측 확률 반환\n",
    "    - decision_function() : z값 반환, 선형방정식의 출력 반환\n",
    "  - SGDClassifier() : 확률적 경사 하강법을 사용한 분류 모델\n",
    "    - loss : 최적화할 손실 함수 지정\n",
    "    - max_iter : 에포크 횟수 지정\n",
    "  - DecisionTreeClassifier() : 결정 트리 분류 클래스\n",
    "    - max_depth : 트리가 성장할 최대 깊이\n",
    "    - feature_importances : 특성 중요도\n",
    "  - plot_tree() : 결정 트리 모델 시각화\n",
    "  - cross_validate(검증할 모델 객체, 특성, 타겟) : 교차 검증 수행 함수\n",
    "    - cv : 교차 검증 폴드 수/splitter 객체 지정\n",
    "    - n_jobs : 1이면 하나의 cpu 코어 사용, -1이면 시스템의 모든 코어 사용\n",
    "  - GridSearchCV(탐색할 모델 객체, 탐색할 모델의 매개변수와 값) : 교차 검증으로 하이퍼파라미터 탐색 수행, 최상의 모델을 찾은 후 훈련 세트 전체를 사용해 최종 모델을 훈련\n",
    "    - best_params : 그리드 서치로 찾은 최적의 매개변수\n",
    "  - RandomizedSearchCV() : 연속된 매개변수 값을 탐색할 때 유용, 샘플링할 수 있는 확률 분포 객체를 전달\n",
    "  - RandomForestClassifier() : 랜덤 포레스트 분류 클래스\n",
    "    - oob_score : OOB 샘플을 사용해 훈련한 모델을 평가할지 지정\n",
    "  - ExtraTreesClassifier() : 엑스트라 트리 분류 클래스\n",
    "  - GradientBoostingClassifier() : 그레이디언트 부스팅 분류 클래스\n",
    "    - n_estimators : 부스팅 단계를 수행하는 트리 수\n",
    "    - learning_rate : 트리가 앙상블에 기여하는 정도\n",
    "  - HistGradientBoostingClassifier(), XGBClassifier(), LGBMClassifier() : 히스트그램 기반 그레이디언트 부스팅 분류 클래스\n",
    "  - permutation_importance() : 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰해 어떤 특성이 중요한지 계산\n",
    "  - KMeans : K-평균 알고리즘 클래스\n",
    "    - n_clusters : 클러스터 개수 지정\n",
    "    - n_init : 반복 횟수 지정\n",
    "    - max_iter : 알고리즘 최대 반복 횟수\n",
    "    - labels_ : 군집된 결과 저장\n",
    "  - PCA : 주성분 분석을 수행하는 클래스\n",
    "    - n_components : 주성분 개수 지정\n",
    "    - explained_variance_ : 설명된 분산 지정\n",
    "    - explained_variance_ratio_ : 설명된 분산의 비율 지정\n",
    "    - inversed_transform() : transform()으로 차원을 축소시킨 데이터 복원\n",
    "\n",
    "- numpy\n",
    "  - seed() : 난수 생성을 위한 정수 초깃값 지정, 초기값이 같으면 동일한 난수 추출 가능\n",
    "  - shuffle(): 주어진 배열을 랜덤하게 섞음\n",
    "  - arrange() : 일정한 간격의 정수/실수 배열 생성\n",
    "  - column_stack(), concatenate() : 전달받은 리스트들을 연결\n",
    "  - reshape() : 배열의 크기 변환\n",
    "  - load() : 넘파이에 파일을 로드\n",
    "\n",
    "- scipy\n",
    "  - expit() : 시그모이드 함수\n",
    "  - uniform() : 주어진 범위에서 실숫값을 무작위로 뽑음\n",
    "  - \brandint() : 주어진 범위에서 정숫값을 무작위로 뽑음\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**실습**\n",
    "\n",
    "- [k-최근접 이웃 알고리즘](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/BreamAndSmelt.ipynb)\n",
    "- [훈련 세트와 테스트 세트](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/training_test_data.ipynb)\n",
    "- [데이터 전처리](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/data_preprocessing.ipynb)\n",
    "- [k-최근접 이웃 회귀](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/regression.ipynb)\n",
    "- [선형 회귀](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/linear_regression.ipynb)\n",
    "- [다중 회귀_특성 공학](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/multiple_regression.ipynb)\n",
    "- [로지스틱 회귀](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/logistic_regression.ipynb)\n",
    "- [확률적 경사 하강법](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/stochastic_Gradient_Descent.ipynb)\n",
    "- [결정 트리](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/decision_tree.ipynb)\n",
    "- [교차 검증과 그리드 서치](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/cross_grid.ipynb)\n",
    "- [앙상블 학습](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/ensemble_learning.ipynb)\n",
    "- [군집 알고리즘](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/clustering.ipynb)\n",
    "- [k-평균](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/k_means.ipynb)\n",
    "- [주성분 분석](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/MachineLearning/pca.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mieVNkmDoI0y"
   },
   "source": [
    "# ***Deep Learning***\n",
    "\n",
    "**용어**\n",
    "- 출력층(output layer) : 신경망의 최종 값(z)을 만듦, 이를 바탕으로 클래스를 예측\n",
    "- 뉴런,유닛 : z값을 계산하는 단위\n",
    "- 밀집층(dense layer) : 인공 신경망의 층, 뉴런들이 모두 연결되어 있기에 완전 연결층(fully connected layer)\n",
    "- 활성화 함수(activation function) : 뉴런의 선형 방정식 계산 결과에 적용되는 함수\n",
    "- 원-핫 인코딩(one-hot encoding) : 타깃값을 해당 클래스만 1이고 나머지는 모두 0인 배열로 만드는 것\n",
    "- 은닉층(hidden layer) : 입력층과 출력층 사이에 있는 모든 층\n",
    "- 렐루 함수(ReLU) : 이미지 분류 모델의 은닉층에서 사용, 입력이 양수일 경우 통과, 음수일 경우 0\n",
    "- 적응적 학습률(adaptive learning rate) : 모델이 최적점에 가까울수록 학습률 하락-> 안정적으로 최적점에 수렴\n",
    "- 콜백(callback) : 훈련 과정 중간에 어떤 작업을 수행할 수 있게 하는 객체\n",
    "- 조기 종료(early stopping) : 검증 점수가 더이상 감소하지 않고 상승해 과대적합이 발생하면 멈춤\n",
    "- 필터,커널 : 밀집층의 뉴런에 해당\n",
    "  - 필터 : 뉴런의 개수\n",
    "  - 커널 : 입력에 곱해지는 가중치\n",
    "- 특성맵 : 합성곱 계산을 통해 얻은 출력\n",
    "- 패딩 : 입력 배열의 주위를 가상 원소로 채우는 것\n",
    "  - same padding : 입력과 특성맵의 크기를 동일하게 만들기 위해 입력 주위를 0으로 채움\n",
    "  - valid padding : 패딩 없이 순수 입력 배열에서만 합성곱을 해 특성맵 생성\n",
    "- 스트라이드(stride) : 필터의 이동 크기\n",
    "- 풀링(pooling) : 특성맵의 가로세로 크기를 줄이는 역할, 가중치 없음, 가로세로 방향으로만 진행, 특성맵의 개수는 동일\n",
    "  - 최대 풀링(max pooling) : 최댓값을 고름\n",
    "  - 평균 풀링(mean pooling) : 평균값을 고름\n",
    "- 함수형 API : 케라스 신경망 모델 생성 방법, Model클래스에 입출력 지정\n",
    "- 순차 데이터(sequential data) : 순서에 의미가 있는 데이터 ex) 시계열 데이터\n",
    "- 타임스텝(timestep) : 샘플을 처리하는 한 단계\n",
    "- 셀(cell) : 순환 신경망에서의 층\n",
    "- 은닉 상태(hidden state) : 셀의 출력\n",
    "- 순환층 : 마지막 타임스텝의 출력만 다음 층으로 전달\n",
    "- 자연어 처리(Natural Language Processing, NLP) : 컴퓨터를 사용해 인간의 언어 처리\n",
    "  - 말뭉치(corpus) : 훈련 데이터\n",
    "- 토큰(token) : 텍스트에서 공백으로 구분되는 문자열\n",
    "- 셀 상태(cell state) : 다음 층으로 전달되지 않고 LSTM셀에서 순환만 함\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**알고리즘**\n",
    "- 인공 신경망(Artificial Neural Network) : 뉴런에서 영감 받아 만든 머신러닝 알고리즘, 딥러닝\n",
    "  - 최적화 대상 : 손실\n",
    "- 심층 신경망(Deep Neural Network) : 2개 이상의 층을 포함한 신경망, 다층 인공 신경망, 딥러닝\n",
    "- 옵티마이저(optimizer) : 신경망의 가중치와 절편을 학습하기 위한 알고리즘, 경사 하강법 알고리즘(SGD, 네스테로프 모멘텀, RMSprop, Adam)\n",
    "- 드롭아웃(dropout) : 은닉층의 뉴런의 출력을 랜덤하게 꺼서 과대적합을 막는 기법, 훈련에만 적용\n",
    "- 완전 연결 신경망(fully connected neural network) : 밀집 신경망, 완전 연결 층(밀집층)만을 사용해 만든 신경망\n",
    "- 합성곱(convolution) : [입력*가중치 + 절편]하는 선형 계산, 입력의 일부만 사용\n",
    "- 합성곱 신경망(Convolutional Neural Network) : 1개 이상의 합성곱 층을 쓴 인공 신경망\n",
    "- 합성곱 신경망의 시각화\n",
    "  - 가중치 시각화 : 합성곱 층의 가중치를 이미지로 출력\n",
    "  - 특성 맵 시각화 : 합성곱 층의 활성화 출력을 이미지로 그리는 것\n",
    "- 피드포워드 신경망(feedforward neural network) : 입력 데이터의 흐름이 앞으로만 전달되는 신경망\n",
    "  - 완전 연결 신경망, 합성곱 신경망\n",
    "- 순환 신경망(recurrent neural network) : 완전 연결 신경망 + 이전 데이터의 처리 순환 고리 : 순차 데이터에 잘 맞는 인공 신경망\n",
    "- 단어 임베딩(word embedding) : 단어를 고정된 크기의 실수 벡터로 변환\n",
    "- LSTM(Long Short-Term Memory) 셀 : 타임스텝이 긴 데이터를 학습하기 위한 순환층\n",
    "  - 입력 게이트 : 새로운 정보 셀 상태에 추가\n",
    "  - 삭제 게이트 : 셀 상테에 있는 정보 제거\n",
    "  - 출력 게이트 : 셀 상태가 다음 은닉 상태로 출력\n",
    "- GRU(Gated Recurrent Unit) 셀 : LSTM셀의 간소화 버전\n",
    "  - 3개 셀 = 2개(*sigmoid) + 1개(*tanh)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**패키지**\n",
    "- tensorflow : keras의 백엔드\n",
    "  - keras\n",
    "    - Dense(뉴런 개수, 활성화 함수, 입력의 크기) : 밀집층 생성 클래스\n",
    "    - Sequential() : 신경망 모델 생성 클래스\n",
    "    - compile(손실 함수, 측정 지표) : 모델 객체 훈련 전 설정 단계\n",
    "    - add() : 모델에 층 추가 메서드\n",
    "    - Flatten() : 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할\n",
    "    - Dropout() : 드롭아웃 층\n",
    "    - save_weights() : 모든 층의 가중치와 절편을 파일에 저장\n",
    "    - load_weights() : 모든 층의 가중치와 절편을 파일에 읽음\n",
    "    - save() : 모델 구조, 모든 가중치와 절편을 파일에 저장\n",
    "    - load_model() : save()로 저장된 모델 로드\n",
    "    - EarlyStopping() : 조기 종료\n",
    "    - Conv2D(필터 수, 커널 크기) : 입력의 너비와 높이 방향의 합성곱 연산 클래스\n",
    "      - strides : 필터의 이동 간격\n",
    "      - activation : 합성곱 층에 적용할 활성화 함수\n",
    "    -  MaxPooling2D(풀링 크기) : 입력의 너비와 높이를 줄이는 풀링 연산 클래스\n",
    "    - plot_model() : 케라스 모델 구조를 그림으로 표현\n",
    "    - Model(입력,출력) : 케라스 모델 생성 클래스\n",
    "    - pad_sequences() : 시퀀스 길이를 맞추기 위한 패딩 추가\n",
    "      - padding : pre-시퀀스 앞에 패딩 추가, post-시퀀스 뒤에 패딩 추가\n",
    "    - SimpleRNN(뉴런 수) : 기본 순환층 생성 클래스\n",
    "    - to_categorical() : 정수 시퀀스를 원-핫 인코딩으로 변환\n",
    "    - Embedding(어휘사전 크기, 임베딩 벡터 크기) : 단어 임베딩을 위한 클래스\n",
    "    - LSTM(뉴런 수) : LSTM셀을 사용한 순환층 클래스\n",
    "    - GRU(뉴런 수) : GRU셀을 사용한 순환층 클래스\n",
    "\n",
    "- numpy\n",
    "  - argmax() : 배열 축에 따라 최댓값의 인덱스 반환\n",
    "\n",
    "- matplotlib\n",
    "  - pyplot\n",
    "    - imshow() : 픽셀의 강도 표현\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**실습**\n",
    "- [인공 신경망](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/DeepLearning/artificial_neural_network.ipynb)\n",
    "- [심층 신경망](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/DeepLearning/deep_neurarl_network.ipynb)\n",
    "- [신경망 모델 훈련](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/DeepLearning/neural_network_training.ipynb)\n",
    "- [합성곱 신경망](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/DeepLearning/convolutional_neural_network.ipynb)\n",
    "- [순환신경망으로 IMDB 리뷰 분류](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/DeepLearning/rnn_imdb.ipynb)\n",
    "- [LSTM과 GRU셀](https://github.com/Wonyoungpark/TIL/blob/main/ML_DL/DeepLearning/lstm_gru.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaFs8FaO1I0TBljY48xY37",
   "collapsed_sections": [],
   "name": "mldl_md.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
